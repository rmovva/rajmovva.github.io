---
layout: default
---

<img class="profile-picture" src="images/profile.jpg">

Hi, I’m Raj, a CS PhD student at Cornell Tech. I am advised by [Emma Pierson](https://www.cs.cornell.edu/~emmapierson/) and supported by an NSF graduate fellowship. I'm broadly interested in the social implications of machine learning, usually focusing on empirical questions in NLP and healthcare. My recent projects study how [language modeling research is becoming increasingly sociotechnical](https://arxiv.org/abs/2307.10700) and how [coarse race categories undermine algorithmic fairness audits](https://arxiv.org/abs/2304.09270).

Previously, I studied CS at MIT with minors in Women’s & Gender Studies and Biology. I was lucky to work with the [Data + Feminism Lab](https://dataplusfeminism.mit.edu/), where we developed intersectional, participatory NLP systems to support activist labor ([FAccT 2022](https://dl.acm.org/doi/10.1145/3531146.3533132)). I interned with Apple's Siri Research team, studying efficiency and compression for language models \[[1](https://aclanthology.org/2022.coling-1.252/), [2](https://www.aclweb.org/anthology/2020.blackboxnlp-1.19/)\]. I am very thankful to my excellent mentors along the way, including Harini Suresh, Jonathan Frankle, and Shayne Longpre; see [here](https://rajivmovva.com/people) for a longer list of people who I'm indebted to! I was also president of [AI@MIT](https://www.ai-at-mit.com/), where I helped design [intro ML workshops](https://github.com/AI-at-MIT/Workshops) and ordered lots of food.

I hew closely to PhD stereotypes: my primary hobbies are baking, rock climbing, [reading](https://www.goodreads.com/user/show/139600509-rajiv-movva), mixology, and tennis. If you'd like to read some unreliable recipes I've written, see [here](https://rajivmovva.com/recipes).  

### Updates

- [2023-12] Our paper on granular race disparities won a Best Paper Honorable Mention at [ML4H 2023](https://ml4h.cc/2023/)!
- [2023-11] Data (for 17K LLM papers through Sep 2023) and code for the LLM survey paper are [now available on Github](https://github.com/rmovva/LLM-publication-patterns-public)! I'll be presenting a poster at [TADA 2023](https://tada2023.org/).  
- [2023-08] I presented our [paper on granular race disparities](https://arxiv.org/abs/2304.09270) at MLHC 2023! Thanks for the excellent conversations; code is available [on Github](https://github.com/rmovva/granular-race-disparities_MLHC23).
- [2023-07] [New working paper](https://arxiv.org/abs/2307.10700) on arXiv! We analyzed 14K language modeling-related papers on arXiv to detail recent publishing trends.
- [2023-04] [Our new paper](https://arxiv.org/abs/2304.09270) is on arXiv, the first from my PhD! We find that granular race categories are critical to algorithmic fairness analyses in healthcare; [here's](https://twitter.com/rajivmovva/status/1651237859465080834) a summary.
- [2022-08] [Our paper](https://arxiv.org/abs/2208.09684) on compressing language models is accepted to COLING 2022! I led this project during an Apple internship in 2021.
<!-- - [2022-04] In the Fall, I'll start my PhD at Cornell Tech in NYC. -->
<!-- - [2022-04] I was awarded an NSF Graduate Fellowship! -->

<!-- Though it's no longer my main interest, I'm also passionate about computational biology, including [functional epigenomics](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0218073) and ligand-protein binding prediction. My favorite hobby is cooking, along with other stereotypical grad student activities: lifting weights, baking, [reading](https://www.goodreads.com/user/show/139600509-rajiv-movva), and playing tennis. You can find some of my [recipes](https://rajivmovva.com/recipes) here (it's a WIP). -->

<!-- While there, I worked with Prof. Catherine D’Ignazio and student Harini Suresh at the [Data + Feminism Lab](https://dataplusfeminism.mit.edu/). Collaborating with activist groups, we co-designed NLP models to support the difficult labor of tracking gender-based violence ([Best Paper, FAccT 2022](https://dl.acm.org/doi/10.1145/3531146.3533132)). The project taught me that naive ML systems often fail at the margins – it takes effort and care to design models for specific, intersectional contexts. -->

<!-- Before that, I explored neural network compression, i.e. improving memory & compute efficiency to mitigate AI’s consumptive footprint. Mentored by Jonathan Frankle, I tested an approach for [parallelized pruning of neural networks](https://arxiv.org/abs/2104.14753). During an internship at Apple, I [combined compression techniques](https://aclanthology.org/2022.coling-1.252/) to rein in the compute footprint of large language models. I also earned Best Paper at BlackboxNLP 2020 for studying how [pruning affects interpretability](https://www.aclweb.org/anthology/2020.blackboxnlp-1.19/) in Transformers.  -->

<!-- Though it's no longer my main interest, I'm also passionate about computational biology, including [functional epigenomics](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0218073) and ligand-protein binding prediction. My favorite hobby is cooking, along with other stereotypical grad student activities: lifting weights, baking, [reading](https://www.goodreads.com/user/show/139600509-rajiv-movva), and playing tennis. You can find some of my [recipes](https://rajivmovva.com/recipes) here (it's a WIP). -->
